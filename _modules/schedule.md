---
title: Paper reading list and presenters
---

Session 1
: Introduction and Motivation (May 24, 19:00 - 19:50)
  : Boqing Gong, Chen Sun
: 1. [Motivation Slides](https://drive.google.com/file/d/1cfCWxycGGuZMcrcE2IJJBmG9UMkyJ7ka/view?usp=sharing)
  1. [Introduction Prismia](https://prismia.chat/shared/J2AE-A8GP)

Session 2
: Recurrent Networks, Attention, Transformers (May 24, 20:00 - 21:30)
  : Boqing Gong, Chen Sun
: 1. [RNN, Attention Prismia](https://prismia.chat/shared/HLQN-3X31)
  1. [Transformers Slides](https://drive.google.com/file/d/1V5bq4cYinhRzD6Ucki8pEo-fmKt6RoiN/view?usp=sharing)
  1. [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
  1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  1. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)

Session 3
: Transformers for Vision and Long Sequences (May 24, 21:30 - 23:00)
  : Boqing Gong, Chen Sun
: 1. [Vision Transformer Prismia](https://prismia.chat/shared/3D2J-FIPK)
  1. [Transformer for Long Sequences Slides](https://drive.google.com/file/d/1x_MdqDDw00VnXrkmeAveza1t9Dd7by_-/view?usp=sharing)
  1. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
  1. [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)
  1. [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)
  1. [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
  1. [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)
  1. [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)

Session 4
: Optimization for Transformers (May 25, 19:00 - 19:50)
  : Boqing Gong
: 1. [Prismia](https://prismia.chat/shared/6JG3-5LA6)
  1. [When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations](https://arxiv.org/abs/2106.01548)
  1. [Surrogate Gap Minimization Improves Sharpness-Aware Training](https://openreview.net/forum?id=edONMAnhLu-)

Session 5
: Transformers for Decision Making (May 25, 20:00 - 20:50)
  : Chen Sun
: 1. [Slides](https://drive.google.com/file/d/18jcH-4FXPmrxLl5MDmbB3sT3-WQOFwso/view?usp=sharing)
  1. [Recording](https://drive.google.com/file/d/1HIZk9YbKbGzuromC1gDgvngPExs56pNi/view?usp=sharing)
  1. [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)
  1. [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://trajectory-transformer.github.io/)
  1. [VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation](https://arxiv.org/abs/2005.04259)
  1. [Episodic Transformer for Vision-and-Language Navigation](https://arxiv.org/abs/2105.06453)

Session 6
: Multimodal Transformers (May 26, 19:00 - 20:50)
  : Boqing Gong, Chen Sun
: 1. [Chen's slides](https://drive.google.com/file/d/1YC3J3HJcErgDiRXzXCiQw5NxVWPvUZxz/view?usp=sharing)
  1. [Boqing's slides](https://docs.google.com/presentation/d/1HawJsoLEBZfpC3nQH6lXv8oYIh92V2gjcMHnMYGsJGU/edit?usp=sharing&resourcekey=0-TjWZEEa_uAgiNn_NFTf5rg)
  1. [Recording](https://drive.google.com/file/d/1LuTo9O_gY4Vi13SFrSLMerejrJAW-iCq/view?usp=sharing)
  1. [Attention Bottlenecks for Multimodal Fusion](https://arxiv.org/abs/2107.00135)
  1. [VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766)
  1. [VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text](https://arxiv.org/abs/2104.11178)
  1. [CLIP: Connecting Text and Images](https://openai.com/blog/clip/)
  1. [Learning Temporal Dynamics from Cycles in Narrated Video](https://arxiv.org/abs/2101.02337)

Session 7
: Model Interpretability (May 26, 21:00 - 21:50)
  : Chen Sun
: 1. [Slides](https://drive.google.com/file/d/1MkdSXUherW8MC6DVrmkWzk9vWOiyf1YW/view?usp=sharing)
  1. [Recording](https://drive.google.com/file/d/1bTtxmS5Nn1HWfUpAPhAeZDUmwwIT-O62/view?usp=sharing)
  1. [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)
  1. [BERT Rediscovers the Classical NLP Pipeline](https://arxiv.org/abs/1905.05950)
  1. [Do Vision-Language Pretrained Models Learn Primitive Concepts?](https://arxiv.org/abs/2203.17271)
  1. [Does Vision-and-Language Pretraining Improve Lexical Grounding?](https://arxiv.org/abs/2109.10246)

Session 8
: Advanced Topics, Recap (May 26, 22:00 - 22:50)
  : Boqing Gong
: 1. [Slides](https://docs.google.com/presentation/d/1Cc7nXVUsGIBgoVYZ7bPGfo1eV1_9owBl2DIiNMHc7kA/edit?usp=sharing&resourcekey=0-pacLpZfAVg-f31BSVYD7PQ)
  1. [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)